{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce9af700",
   "metadata": {},
   "source": [
    "# Aufgabe\n",
    "\n",
    "Wählt zwei beliebige (englische) Texte aus, von denen ihr erwartet, dass einer etwas \"unwahrscheinlicher\" für ein Sprachmodell ist als der andere (z.B. ein Gedicht vs. einen Zeitungsartikel). Berechnet die Perplexity des BERT-Modells für beide Texte. Entspricht das Ergebnis eurer Hypothese?\n",
    "\n",
    "(Gerne könnt ihr die Aufgabe auch für deutsche Texte machen -- dann müsste ihr auf huggingface nach einem deutschen BERT-Modell suchen.)\n",
    "\n",
    "Siehe dazu auch: https://huggingface.co/docs/transformers/perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af4145a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the texts from the /texts folder\n",
    "import os\n",
    "import torch\n",
    "\n",
    "texts = {}\n",
    "for file in os.listdir('texts'):\n",
    "    with open('texts/'+file) as f:\n",
    "        texts[file] = f.read()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1344578a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function was taken from the linked huggingface tutorial https://huggingface.co/docs/transformers/perplexity\n",
    "\n",
    "def compute_perplexity(model, encodings, device):\n",
    "    #get the max length of the bert model\n",
    "    max_length = model.config.max_position_embeddings\n",
    "    stride = 512\n",
    "    seq_len = encodings.input_ids.size(1)\n",
    "\n",
    "    nlls = []\n",
    "    prev_end_loc = 0\n",
    "    for begin_loc in range(0, seq_len, stride):\n",
    "        end_loc = min(begin_loc + max_length, seq_len)\n",
    "        trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
    "        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[:, :-trg_len] = -100\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=target_ids)\n",
    "\n",
    "            # loss is calculated using CrossEntropyLoss which averages over valid labels\n",
    "            # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels\n",
    "            # to the left by 1.\n",
    "            neg_log_likelihood = outputs.loss\n",
    "\n",
    "        nlls.append(neg_log_likelihood)\n",
    "\n",
    "        prev_end_loc = end_loc\n",
    "        if end_loc == seq_len:\n",
    "            break\n",
    "\n",
    "    ppl = torch.exp(torch.stack(nlls).mean())\n",
    "    return ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "69e9b21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#compute the perplexity of the texts using the bert model\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "89159a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "battle_hymn_of_the_republic.txt perplexity: 1.45\n",
      "compcert_c_compiler.txt perplexity: 1.21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\theav\\miniconda3\\envs\\sprachverarbeitung\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:435: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "# Compute perplexity for each text\n",
    "perplexities = {}\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "for name, text in texts.items():\n",
    "    encodings = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "    perplexities[name] = compute_perplexity(model, encodings, device).item()\n",
    "    print(f'{name} perplexity: {perplexities[name]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d1f816",
   "metadata": {},
   "source": [
    "# Antwor\n",
    "\n",
    "Bei den beiden ausgewählten Texten handel es sich um ein Gedicht und einen Hacker News Artikel. Das Gedicht ist \"Battle-Hymn of the Republic\" von Julia Ward Howe und der Hacker News Artikel ist \"The CompCert C compiler\" von der compcert.org Webseite.\n",
    "\n",
    "Meine Erwartung war, dass das Gedicht eine höhere Perplexity hat als der Hacker News Artikel, da das Gedicht in einem älteren Englisch geschrieben ist und dich nicht explizit auf eine perfekte Grammatik konzentriert. Der Hacker News Artikel hingegen ist in einem modernen Englisch geschrieben und sollte daher eine geringere Perplexity haben.\n",
    "\n",
    "Die Perplexity des Gedichts beträgt 1.45 und die des Hacker News Artikels beträgt 1.21\n",
    "Damit ist meine Hypothese bestätigt."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
